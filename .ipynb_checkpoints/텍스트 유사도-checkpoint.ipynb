{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 유사도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## (1) 수치(벡터) 거리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - 수치(벡터) 거리\n",
    "# 1. 코사인 유사도(Cosine Similarity)\n",
    "# 2. 유클리디언 거리 유사도(Euclidean Distance Similarity)\n",
    "# 3. 맨하탄 거리 유사도(Manhattan Distance Similarity)\n",
    "# 4. TS / SS / TS-SS (삼각형/부채꼴/혼합형)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) 편집 거리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - 편집 거리\n",
    "# 1. 해밍 거리 유사도(Hamming Distance Similarity)\n",
    "# 2. 자로윙클러 유사도(Jaro-Winkler Similarity)\n",
    "# 3. 레벤슈타인 유사도(levenshtein Similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 코사인 유사도\n",
    "# - 두 문장을 각각 벡터값으로 변환 후 코사인 각도를 구해 유사도 측정\n",
    "# - 결과값 -1 ~ 1 : 1에 가까울수록 유사도가 높다고 해석\n",
    "# - 동일한 문장 길이가 아니어도 문장 길이를 정규화하여 비교하는 로직"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 유클리디언 거리 유사도\n",
    "# - 두 문장을 각각 벡터값으로 변환 후 n차원 공간에서 두 점 사이 최단거리를 구해 유사도 측정\n",
    "# - 결과값 0 ~ 무한대 : 0에 가까울수록 유사도가 높다고 해석\n",
    "# - 결과값을 0 ~ 1 사이의 값으로 정규화 시켜 해석 진행\n",
    "# - 수치 간 거리를 구하는 방식에는 좋지만, 정보간의 거리를 나타내는 것에는 좋지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 맨하탄 거리 유사도\n",
    "# - 유클리디언과 비슷한 로직이지만, 두 점 사이의 직선이 아닌 블록 단위의 최단거리를 구해 유사도 측정\n",
    "# - 타 알고리즘에 비해 정확도가 떨어진다는 의견이 많음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 해밍 거리 유사도\n",
    "# - 두 개의 텍스트의 길이가 같을 때, 각 문자가 서로 다를 때마다 거리를 1씩 늘려나가는 방식으로 계산.\n",
    "#   즉, 같은 문자로 바꾸기 위해 몇 개의 글자를 바꿔줘야 하는지에 대한 계산 방식\n",
    "# - 결과값 0 ~ 무한대 : 0에 가까울수록 유사도가 높다고 해석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 자로윙클러 유사도\n",
    "# - 두 텍스트간 편집 횟수를 카운팅(Transposition만 고려)\n",
    "# - 결과값 0 ~ 1 : 1에 가까울수록 유사도가 높다고 해석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 레벤슈타인 유사도\n",
    "# - 두 텍스트간 편집 횟수를 카운팅(Transposition뿐만 아닌 삭제, 수정 등 고려)\n",
    "# - 결과값 0 ~ 무한대 : 0에 가까울수록 유사도가 높다고 해석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - 편집 거리의 유사도 고려사항\n",
    "# ex. 미르건축사무소 & 아르건축사무소 => 유사도 80%\n",
    "#     (전처리 후) 미르 & 아르 => 유사도 50%\n",
    "# 즉, (전처리 이후 단어가 작아지므로) 한 단어라도 일치할 경우 편집 거리의 유사도는 높게 출력됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhive import hive\n",
    "from os import linesep, path, sep\n",
    "from requests import Session\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.exceptions import *\n",
    "from sys import stdin, stderr, argv\n",
    "from urllib.parse import quote_plus\n",
    "import json\n",
    "import traceback\n",
    "import sys\n",
    "import pandas as pd\n",
    "import time\n",
    "import ctypes\n",
    "import multiprocessing\n",
    "import re\n",
    "from pyhive import hive\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from multiprocessing import Process, current_process, Pool\n",
    "from functools import reduce\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "import affinegap as affi\n",
    "import jellyfish as jell\n",
    "import textdistance as td\n",
    "# from korean_romanizer.romanizer import Romanizer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.type import *\n",
    "from pyspark.sql.functions import pandas_udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "import os\n",
    "from os.path import join, abspath\n",
    "import datetime\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from dateutil import relativedelta\n",
    "import dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Spark_Session(object):\n",
    "    \n",
    "#     \"\"\"\n",
    "#         Spark Session 생성 Class\n",
    "#         Args:\n",
    "#         Returns:\n",
    "#         Exception:\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self,\n",
    "#                 appname,\n",
    "#                 config_dict = None,\n",
    "#                 dynamic = True\n",
    "#                 verbose = False):\n",
    "#         \"\"\"\n",
    "#             Spark_Session Class의 생성자\n",
    "            \n",
    "#             예시)\n",
    "#                 1) 일반적으로 spark 세션을 분석 환경에서 열어서 사용하고 싶을 때\n",
    "#                     (* start() Method를 사용해야함)\n",
    "#                     spark = Spark_Session(appname = \"DonationDeduction\", config_dict = config_dict, dynamic = False).start()\n",
    "                    \n",
    "#                 2) Context Manager를 사용해 with 구문 안에서 작업하고 싶을 때\n",
    "#                     (* start() Method를 사용하지 않아도 됨)\n",
    "#                     with Spark_Session(appname = \"DonationDeduction\", config_dict = config_dict, verbose = True) as sp:\n",
    "#                     sp.sql(\"select * from tmpr.table_nm limit 100\").show()\n",
    "                    \n",
    "#                     Args:\n",
    "#                         appname : 사용할 App Name (String)\n",
    "#                         config_dict : Spark Config 관련 정보 (Dictionary)\n",
    "#                             ex) config_dict = {\"spark.driver.memory\" : \"32g\",\n",
    "#                                                 \"spark.executor.instances\" : \"8\",\n",
    "#                                                 \"spark.executor.cores\" : \"8\",\n",
    "#                                                 \"spark.executor.memory \" : \"32g\"}\n",
    "#                         dynamic : dynamicAllocation을 사용할지 여부(Logical)\n",
    "#                         verbose : 만들어진 소스 코드를 출력할지 여부(Logical)\n",
    "                        \n",
    "#                     Returns:\n",
    "#                         Spark Session을 시작하는 코드(String)\n",
    "#                     Exception: None\n",
    "#         \"\"\"\n",
    "        \n",
    "#         self.appname = appname\n",
    "#         self.config_dict = config_dict\n",
    "#         self.verbose = verbose\n",
    "#         self.dynamic = dynamic\n",
    "        \n",
    "#         # set environment_variable\n",
    "#         self.driverHost = os.environ[\"NODEPORT_IP\"]\n",
    "#         self.driverPort = os.environ[\"SERVICE_PORT_SPARK1\"]\n",
    "#         self.blockMngPort = os.environ[\"SERVICE_PORT_SPARK2\"]\n",
    "#         self.driverBindAddress = \"0.0.0.0\"\n",
    "#         self.warehouse_location = abspath('/warehouse/tablespace/managed/hive')\n",
    "        \n",
    "#         # 주어진 설정 정보를 활용하여 소스 코드를 생성함\n",
    "#         Self.__create_session_start_code()\n",
    "        \n",
    "#     def __create_session_start_code(self):\n",
    "#         \"\"\"\n",
    "#             Spark Session을 시작하는 코드(String) 생성하는 함수\n",
    "#             Args : None\n",
    "#             Returns :\n",
    "#                 Spark Session을 시작하는 코드(String)\n",
    "#             Exception : None\n",
    "#         \"\"\"\n",
    "        \n",
    "#         # Spark Session 설정 코드 생성\n",
    "#         # --> 이후 start Method에서 exec으로 실행\n",
    "#         self.temp_code = \"\"\"_spark = (SparkSession\n",
    "#                                         .builder\n",
    "#                                         .master('yarn')\n",
    "#                                         .appName('\"\"\" + self.appname + \"\"\"')\n",
    "#                                         .config('spark.driver.host', '\"\"\" + self.driverHost + \"\"\"')\n",
    "#                                         .config('spark.driver.port', '\"\"\" + self.driverPort + \"\"\"')\n",
    "#                                         .config('spark.driver.blockManager.port', '\"\"\" + self.blockMngPort + \"\"\"')\n",
    "#                                         .config('spark.driver.bindAddress', '\"\"\" + self.driverBindAddress + \"\"\"')\n",
    "#                                         .config('spark.sql.warehouse.dir', '\"\"\" + self.warehouse_location + \"\"\"')\n",
    "#                                         .config(\"hive.exec.dynamic.partition\", \"true\")\n",
    "#                                         .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "#                                         .config(\"spark.sql.broadcastTimeout\",\"36000\")\n",
    "#                                         )\"\"\"\n",
    "#         # Dynamic Allocation 관련 설정\n",
    "#         if self.dynamic == True:\n",
    "#             dynamic_code = \"\"\".config(\"spark.shuffle.service.enabled\",\"true\")\n",
    "#                                 .config(\"spark.dynamicAllocation.enabled\",\"true\")\n",
    "#                                 .config(\"spark.dynamicAllocation.minExecutors\",\"8\")\n",
    "#                                 .config(\"spark.dynamicAllocation.initialExecutors\",\"12\")\n",
    "#                                 .config(\"spark.dynamicAllocation.maxExecutors\",\"32\")\n",
    "#                             \"\"\"\n",
    "#             self.temp_code = self.temp_code + dynamic_code\n",
    "            \n",
    "#         if self.config_dict is not None:\n",
    "#             for key, value in self.config_dict.items():\n",
    "#                 self.temp_code = self.temp_code + \"\"\".config('\"\"\" + key + \"\"\"','\"\"\" + value + \"\"\"')\\n\"\"\"\n",
    "                \n",
    "#                 self.temp_code = self.temp_code + \"\"\".enableHiveSupport()\n",
    "#                                                     .getOrCreate() )\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
